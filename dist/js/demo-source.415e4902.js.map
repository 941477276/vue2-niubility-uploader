{"version":3,"sources":["webpack:///./src/node-server/server.js"],"names":[],"mappings":"6GAAA,OAAe,iva","file":"js/demo-source.415e4902.js","sourcesContent":["export default \"const express = require('express');\\nconst multer = require('multer');\\nconst path = require('path');\\nconst fs = require('fs');\\nconst cors = require('cors');\\nconst { formidable } = require('formidable');\\n\\n// Create Express app\\nconst app = express();\\nconst PORT = process.env.PORT || 3001;\\n\\n// Enable CORS\\napp.use(cors());\\n\\n// Middleware to parse JSON\\napp.use(express.json({ limit: '50mb' }));\\napp.use(express.urlencoded({ extended: true, limit: '50mb' }));\\n\\n// Create upload directory if it doesn't exist\\nconst uploadDir = path.join(__dirname, 'temp');\\nif (!fs.existsSync(uploadDir)) {\\n  fs.mkdirSync(uploadDir, { recursive: true });\\n}\\n\\n// Temporary directory for chunked uploads\\nconst tempDir = path.join(__dirname, 'chunk-temp');\\nif (!fs.existsSync(tempDir)) {\\n  fs.mkdirSync(tempDir, { recursive: true });\\n}\\n\\n// Configure multer for regular file uploads\\nconst storage = multer.diskStorage({\\n  destination: (req, file, cb) => {\\n    cb(null, uploadDir);\\n  },\\n  filename: (req, file, cb) => {\\n    // Use original filename with timestamp to avoid conflicts\\n    const mimeType = file.mimetype;\\n    const fileName = 'img.' + mimeType.split('/').pop().toLowerCase();\\n    console.log('multer.diskStorage, filename', fileName, file);\\n    const ext = path.extname(file.originalname || fileName);\\n    const name = path.basename(file.originalname || fileName, ext);\\n    const filename = `${name}_${Date.now()}${ext}`;\\n    cb(null, filename);\\n  }\\n});\\n\\nconst upload = multer({\\n  storage: storage,\\n  limits: {\\n    fileSize: 10 * 1024 * 1024 * 1024 // 10GB max file size\\n  }\\n});\\n\\n// In-memory storage for upload sessions (in production, use Redis or database)\\nconst uploadSessions = new Map();\\n\\n\\n/**\\n * GET /health - Health check endpoint\\n */\\napp.get('/health', (req, res) => {\\n  res.json({ status: 'OK', timestamp: new Date().toISOString() });\\n});\\n\\n/**\\n * POST /upload - Single file upload endpoint\\n */\\napp.post('/upload', upload.single('file'), (req, res) => {\\n  try {\\n    if (!req.file) {\\n      return res.status(400).json({ error: 'No file uploaded' });\\n    }\\n\\n    let name = req.name;\\n    // Return success response with file info\\n    res.json({\\n      success: true,\\n      message: 'File uploaded successfully',\\n      file: {\\n        filename: req.file.filename || name,\\n        originalName: req.file.originalname || name,\\n        size: req.file.size,\\n        path: req.file.path\\n      }\\n    });\\n  } catch (error) {\\n    console.error('Upload error:', error);\\n    res.status(500).json({ error: 'Failed to upload file' });\\n  }\\n});\\n\\n/**\\n * POST /upload/init - Initialize a chunked upload session\\n */\\napp.post('/upload/init', async (req, res) => {\\n  try {\\n    const { fileName, fileSize, fileType, uploadId } = req.body;\\n    console.log('/upload/init', req.body);\\n\\n    if (!fileName || !fileSize) {\\n      return res.status(400).json({ error: 'Missing required fields: fileName, fileSize' });\\n    }\\n\\n    // Create session data\\n    const session = {\\n      uploadId,\\n      fileName,\\n      fileSize: parseInt(fileSize),\\n      fileType: fileType || '',\\n      uploadedSize: 0,\\n      totalChunks: 0,\\n      uploadedChunks: new Set(),\\n      createdAt: new Date().toISOString(),\\n      expiresAt: new Date(Date.now() + 60 * 60 * 1000).toISOString(), // 1 hours\\n      tempFilePath: path.join(tempDir, uploadId)\\n    };\\n\\n    const tempFilePath = path.resolve(__dirname, `./chunk-temp/${uploadId}`)\\n    console.log('/upload/init, tempFilePath', fs.existsSync(session.tempFilePath), session.tempFilePath, tempFilePath);\\n    // Create temporary directory for this upload\\n    if (!fs.existsSync(tempFilePath)) {\\n      try {\\n        fs.mkdirSync(tempFilePath, { recursive: true });\\n      } catch (err) {\\n        console.error('创建文件夹失败', err);\\n      }\\n\\n    }\\n\\n    // Store session\\n    uploadSessions.set(uploadId, session);\\n\\n    // Clean up expired sessions periodically\\n    if (uploadSessions.size > 100) { // Clean up if we have too many sessions\\n      console.log('/upload/init 清空session')\\n      const now = Date.now();\\n      for (const [id, session] of uploadSessions) {\\n        if (new Date(session.expiresAt).getTime() < now) {\\n          cleanupUploadSession(id);\\n        }\\n      }\\n    }\\n\\n    // Return session info\\n    res.json({\\n      success: true,\\n      uploadId,\\n      message: 'Upload session initialized successfully'\\n    });\\n\\n  } catch (error) {\\n    console.error('Init upload error:', error);\\n    res.status(500).json({ error: 'Failed to initialize upload session' });\\n  }\\n});\\n\\n/**\\n * 跨分区移动文件\\n * @param sourcePath 源文件地址\\n * @param targetPath 目标文件地址\\n * @returns {Promise<void>}\\n */\\nasync function moveFileAcrossPartitions(sourcePath, targetPath) {\\n  try {\\n    // 确保目标目录存在\\n    const targetDir = path.dirname(targetPath);\\n    fs.mkdirSync(targetDir, { recursive: true });\\n\\n    // 创建可读流和可写流\\n    const readStream = fs.createReadStream(sourcePath);\\n    const writeStream = fs.createWriteStream(targetPath);\\n\\n    // 管道传输数据\\n    await new Promise((resolve, reject) => {\\n      readStream.pipe(writeStream)\\n        .on('finish', resolve)\\n        .on('error', reject);\\n    });\\n\\n    // 删除源文件\\n    fs.unlinkSync(sourcePath);\\n\\n    console.log(`文件移动成功（跨分区），源文件：${sourcePath}，目标文件：${targetPath}`);\\n  } catch (err) {\\n    console.error('移动文件失败:', err);\\n  }\\n}\\n\\napp.post('/upload/chunk', async (req, res) => {\\n  try {\\n\\n    const form = formidable({\\n      multiples: false,\\n      // maxFileSize: 100 * 1024 * 1024 // 100MB\\n    });\\n\\n    form.parse(req, async (err, fields, files) => {\\n      if (err) {\\n        return res.status(500).json({\\n          success: false,\\n          message: '解析表单失败: ' + err.message\\n        });\\n      }\\n\\n      try {\\n        // console.log('fields', fields);\\n        const { uploadId, chunkIndex, filename, chunk, totalChunks } = fields;\\n        const chunkFiles = files.file || [];\\n\\n        const chunkIndexInt = parseInt(chunkIndex[0]);\\n        const totalChunksInt = parseInt(totalChunks[0]);\\n        // console.log('chunkFiles', chunkFiles);\\n        if (chunkFiles.length == 0) {\\n          return res.status(400).json({\\n            success: false,\\n            message: '未收到分片文件'\\n          });\\n        }\\n\\n\\n        if (!uploadId[0] || isNaN(chunkIndexInt) || isNaN(totalChunks)) {\\n          return res.status(400).json({ error: 'Missing required fields: uploadId, chunkIndex, totalChunks' });\\n        }\\n\\n        // Check if upload session exists\\n        const session = uploadSessions.get(uploadId[0]);\\n        if (!session) {\\n          return res.status(404).json({ error: 'Upload session not found' });\\n        }\\n\\n        // Check if chunk was already uploaded\\n        if (session.uploadedChunks.has(chunkIndexInt)) {\\n          return res.json({\\n            success: true,\\n            message: 'Chunk already uploaded',\\n            chunkIndex: chunkIndexInt,\\n            status: 'duplicate'\\n          });\\n        }\\n\\n        // 移动临时文件到目标位置\\n        const chunkPath = path.join(session.tempFilePath, `chunk_${chunkIndexInt}.tmp`);\\n        // fs.renameSync(chunkFiles[0].filepath, chunkPath);\\n        await moveFileAcrossPartitions(chunkFiles[0].filepath, chunkPath);\\n\\n\\n        // Update session with chunk info\\n        session.uploadedChunks.add(chunkIndexInt);\\n        // session.uploadedSize += req.file.size;\\n        session.uploadedSize += chunkFiles[0].length || 0;\\n        session.totalChunks = totalChunksInt;\\n\\n        // Update expiration time\\n        session.expiresAt = new Date(Date.now() + 60 * 60 * 1000).toISOString();\\n\\n        // Return success response\\n        res.json({\\n          success: true,\\n          message: 'Chunk uploaded successfully',\\n          chunkIndex: chunkIndexInt,\\n          totalChunks: totalChunksInt,\\n          uploadedSize: session.uploadedSize,\\n          progress: Math.round((session.uploadedSize / session.fileSize) * 100)\\n        });\\n\\n      } catch (error) {\\n        console.error(error);\\n        res.status(500).json({\\n          success: false,\\n          message: '分片上传失败: ' + error.message\\n        });\\n      }\\n    });\\n\\n  } catch (error) {\\n    console.error('Chunk upload error:', error);\\n    res.status(500).json({ error: 'Failed to upload chunk' });\\n  }\\n});\\n\\n/**\\n * POST /upload/finalize - Finalize a chunked upload\\n */\\napp.post('/upload/finalize', async (req, res) => {\\n  try {\\n    const { uploadId, fileName, fileSize } = req.body;\\n\\n    if (!uploadId) {\\n      return res.status(400).json({ error: 'Missing required field: uploadId' });\\n    }\\n\\n    // Check if upload session exists\\n    const session = uploadSessions.get(uploadId);\\n    // console.log('/upload/finalize, session', uploadId, session, uploadSessions);\\n    if (!session) {\\n      return res.status(404).json({ error: 'Upload session not found' });\\n    }\\n\\n    // Verify all chunks were uploaded\\n    if (session.uploadedChunks.size !== session.totalChunks) {\\n      const missingChunks = [];\\n      for (let i = 0; i < session.totalChunks; i++) {\\n        if (!session.uploadedChunks.has(i)) {\\n          missingChunks.push(i);\\n        }\\n      }\\n\\n      return res.status(400).json({\\n        error: 'Not all chunks have been uploaded',\\n        missingChunks,\\n        uploadedChunks: Array.from(session.uploadedChunks),\\n        totalChunks: session.totalChunks\\n      });\\n    }\\n\\n    // Verify file size matches\\n    if (fileSize && parseInt(fileSize) !== session.fileSize) {\\n      return res.status(400).json({\\n        error: 'File size mismatch',\\n        expected: session.fileSize,\\n        actual: fileSize\\n      });\\n    }\\n\\n    // Reassemble the file from chunks\\n    const finalFilePath = path.join(tempDir, session.fileName);\\n    const writeStream = fs.createWriteStream(finalFilePath);\\n\\n    // Sort chunks by index and pipe them in order\\n    const chunkFiles = fs.readdirSync(session.tempFilePath);\\n    const sortedChunks = chunkFiles\\n      .filter(f => f.startsWith('chunk_'))\\n      .sort((a, b) => {\\n        const indexA = parseInt(a.split('_')[1]);\\n        const indexB = parseInt(b.split('_')[1]);\\n        return indexA - indexB;\\n      });\\n\\n    let chunksProcessed = 0;\\n\\n    // console.log('/upload/finalize, sortedChunks', sortedChunks);\\n    // Process each chunk in sequence\\n    for (const chunkFile of sortedChunks) {\\n      const chunkPath = path.join(session.tempFilePath, chunkFile);\\n      const chunkData = fs.readFileSync(chunkPath);\\n\\n      if (!writeStream.write(chunkData)) {\\n        // If the stream wants us to wait, wait until it's ready\\n        await new Promise(resolve => writeStream.once('drain', resolve));\\n      }\\n\\n      chunksProcessed++;\\n    }\\n\\n    // Close the write stream\\n    writeStream.end();\\n\\n    // Wait for the stream to finish writing\\n    await new Promise((resolve, reject) => {\\n      writeStream.on('finish', resolve);\\n      writeStream.on('error', reject);\\n    });\\n\\n    // Verify the final file size\\n    const finalStats = fs.statSync(finalFilePath);\\n    if (finalStats.size !== session.fileSize) {\\n      // Clean up and return error\\n      fs.unlinkSync(finalFilePath);\\n      cleanupUploadSession(uploadId);\\n      return res.status(500).json({\\n        error: 'Final file size does not match expected size',\\n        expected: session.fileSize,\\n        actual: finalStats.size,\\n        finalFilePath\\n      });\\n    }\\n\\n    // Clean up temporary files\\n    cleanupUploadSession(uploadId);\\n\\n    // Return success response\\n    res.json({\\n      success: true,\\n      message: 'File uploaded successfully',\\n      file: {\\n        filename: session.fileName,\\n        size: finalStats.size,\\n        path: finalFilePath\\n      }\\n    });\\n\\n  } catch (error) {\\n    console.error('Finalize upload error:', error);\\n    res.status(500).json({ error: 'Failed to finalize upload' });\\n  }\\n});\\n\\n/**\\n * Clean up upload session and temporary files\\n * @param {string} uploadId - The upload session ID\\n */\\nfunction cleanupUploadSession(uploadId) {\\n  const session = uploadSessions.get(uploadId);\\n  // console.log('cleanupUploadSession', uploadId, session);\\n  if (session) {\\n    // Remove temporary directory\\n    if (fs.existsSync(session.tempFilePath)) {\\n      console.log('cleanupUploadSession删除临时目录', session.tempFilePath);\\n      try {\\n        fs.rmSync(session.tempFilePath, { recursive: true });\\n      } catch (error) {\\n        console.error(`Failed to remove temp directory for ${uploadId}:`, error);\\n      }\\n    }\\n\\n    // Remove session from map\\n    uploadSessions.delete(uploadId);\\n  }\\n}\\n\\n// Periodic cleanup of expired sessions (every hour)\\nsetInterval(() => {\\n  const now = Date.now();\\n  for (const [id, session] of uploadSessions) {\\n    if (new Date(session.expiresAt).getTime() < now) {\\n      console.log(`Cleaning up expired upload session: ${id}`);\\n      cleanupUploadSession(id);\\n    }\\n  }\\n}, 60 * 60 * 1000); // Every hour\\n\\n// Start server\\napp.listen(PORT, () => {\\n  console.log(`服务器运行在端口 ${PORT}`)\\n  console.log(`文件上传服务器运行在端口 ${PORT}`)\\n  console.log(`服务地址: http://localhost:${PORT}`)\\n  console.log(`Upload directory: ${uploadDir}`);\\n  console.log(`Temp directory: ${tempDir}`);\\n  if (!fs.existsSync(tempDir)) {\\n    fs.mkdirSync(tempDir, { recursive: true });\\n  }\\n  if (!fs.existsSync(uploadDir)) {\\n    fs.mkdirSync(uploadDir, { recursive: true });\\n  }\\n});\\n\\nmodule.exports = app;\\n\";"],"sourceRoot":""}